# Final Examination Paper Session 3 2021

## Question 1: Coding

a. Given a list named X which contains words (as strings), implement a Python function to compute the frequencies of words in X. Write down the Python code.

```python
def word_frequency(x):
    freq_dict = {}
    for i in x:
        if i not in freq_dict:
            freq_dict[i] = 1
        else:
            freq_dict[i] += 1
```

b. A list named Y contains numerical values (in float type). But some elements of Y have the value “None” (i.e., a missing value). Implement a Python function which replaces each missing value with the mean value of Y. Write down the Python code.

```python
def replace_with_mean(Y):
    sum = 0
    count = 0
    for i in range(len(Y)):
        if i not "None":
            sum += i
            count += 1
    mean = sum / count
    for i in range(len(Y)):
        if Y[i] == "None":
            Y[i] = mean
    return Y
```

## Question 2: Pre-processing

a. Explain why we cannot reuse the training data for testing in data mining? </br>
We cannot reuse the training data for testing in data mining because the model will be overfitted to the training data and will not be able to generalise to new data. Will be biased towards the training data.

b. Explain the concept of feature selection and feature generation, and in what situation to use this method. </br>
Feature selection is identification and removal of as much irrelevant and redundant information as possible. Goal is to select a subset of features that still describe the original problem. Reduce memory and time requirements, decrease the overfitting and simplify the visualization

Feature generation is the process of creating new attributes that can capture the important information in a dataset more efficiently than the original attributes. Always problem dependent.

c. Explain why we need to convert strings to numerical values in data mining. Describe a concrete example to demonstrate the advantage(s) of one-hot encoding compared with the direct conversion of strings to numerical values.

We need to convert strings to numerical values in data mining because most machine learning algorithms cannot handle strings. One-hot encoding is better than direct conversion of strings to numerical values because it does not imply any ordinal relationship between the categories. For example, if we have a feature called "colour" with values "red", "blue" and "green", one-hot encoding will create three new features "colour_red", "colour_blue" and "colour_green" with values 0 or 1. If we directly convert the strings to numerical values, we will have "colour" with values 1, 2 and 3. This implies that "red" is closer to "blue" than "green", which is not true.

## Question 3: Decision Tree

```
   +----------------------+----+
   |Case|AGE       |Income|Buy?
   +---+-----------+------+----+
   |1   |youth     |high  |yes
   |2   |middleaged|high  |yes
   |3   |senior    |high  |yes
   |4   |senior    |medium|yes
   |5   |youth     |medium|no
   |6   |youth     |medium|no
   |7   |middleaged|low   |no
   |8   |senior    |low   |no
   +---+----------+------+-----+
```

a. Assume that you are given a set of records as shown in the following table, where the last column contains the target variable. Present the procedure of using information gain to identify which attribute should be split. You need to show all steps of your calculation in detail

1. Calculate Entropy

   Entropy(Buy) = -P(no) _ log2(P(no)) - P(yes) _ log2(P(yes))
   Entropy(Buy) = -P(0.5) *log2(0.5) - P(0.5) *log2(0.5)
   Entropy(Buy) = -0.5*(-1) - 0.5*(-1)
   Entropy(Buy) = 1

   Entropy(Age | youth) = -P(2/3)(log2(2/3)) - (P(1/3)(log2(1/3)))
   Entropy(Age | youth) = (-0.67)(-0.58) - (0.33)(-1.59)
   Entropy(Age | youth) = 0.3886 + 0.5283
   Entropy(Age | youth) = 0.9169

   Entropy(Age | senior) = -P(0.33)(log2(0.33)) - (P(0.67)(log2(0.67)))
   Entropy(Age | senior) = -0.33 + 1.59 - 0.67 + 0.578
   Entropy(Age | senior) = 1.168

   Entropy(Age | middleaged) = 1

   WeightedEntropy(Age) = (3/8) \* (0.9169) + (2/8) \_ (1 - 0) + (3/8) \* (1 - 0)
   = 0.34383 + 1/4 + 3/8
   = 0.96

   Entropy(Income | high) = 1

   Entropy(Income | medium) = -P(2/3)(log2(2/3)) - (P(1/3))(log2(1/3))
   = 0.38997 + 0.5283
   = 0.81
   Entropy(Income | low) = -P(0/3)(log2(0/3)) - P(0/3)(log2(1/3))
   = 0

   WeightedEntropy(Income) = (3/8) \* (1) + (3/8) \_ (0.81) + (2/8) \* (1 - 0)
   = 0.375 + 0.304 + 0
   = 0.679

2. Infogain

   Infogain(Age) = 1 - 0.96 = 0.04
   Infogain(Income) = 1 - 0.679 = 0.321

3. Since the IG for attribute 'Income' is higher, we will pick this attribute to splite using info gain as the splitting criteria.

b. Why is ensemble classifier (such as Random Forest) can enhance the performance of individual classifiers? </br>
Helps to reduce overfitting, improved generalisation, more robust to noise, reduction of bias.

## Question 4: Naive Bayes

a. In Naive Bayesian classifiers, the numerical underflow and the zero count are two important issues. Explain these two issues and describe at least one common technique to overcome each issue.

1. If the number of attributes is large, the outputs of a Naïve Bayesian classifier are usually very small.
2. The difference may be close or rounded off to 0 (this is unknown as the underflow problem).

One common technique to overcome the issue:

1. Manipulate the logarithm of a number rather than the number itself.

b. Assume that a Bayesian classifier returns the following outcomes for a binary classification problem,
which are sorted by decreasing probability values. P (resp., N) refers to a record belonging to a positive (resp.,
negative) class.

```
Tuple #     Class   Probability
1           P        0.90
2           P        0.80
3           P        0.70
4           N        0.60
5           P        0.55
6           N        0.54
7           P        0.53
8           N        0.51
9           P        0.50
10          N        0.40
```

Answer the following questions for the above example, and present all steps of calculation in detail.

1.  What is the F score (i.e. F1 score) if setting the probabilistic classification threshold to 0.59? </br>

    ```
    F score = 2 x precision x recall / precision + recall
    Precision = TP/ TP + FP
    Recall = TP/ TP + FN
    TP = 3, FP = 3, FN= 1

    Precision = 3 / 3 + 3 = 0.5
    Recall = 3 / 3 + 1 = 0.75
    F Score = 2 x 0.5 x 0.75 / 0.5 + 0.75 = 0.6
    ```

2.  What is the highest probabilistic classification threshold such that the recall (sensitivity) is at least
    80%?
    ```
    Recall = TP/ TP + FN
    0.8 = 4/ 4 + 1
    Therefore, it is save to assume that in order to achieve the recall of 80%, the threshold should be 0.55. Which achieves a TP of 4 and FN of 1.
    ```

## Question 5: Apache Spark

1. Why is Apache Spark more suitable for data-parallel computation than for model-parallel computation?
   Also use an example to support your answer. </br>
   Apache Spark is better suited for data-parallel computation because its design and distributed framework excel at processing and analyzing large datasets across multiple machines efficiently. It's ideal for tasks like aggregations, filtering, and transformations.

   However, for model-parallel computation, which involves distributing machine learning models and coordinating their training, Spark may face communication overhead challenges. Dedicated deep learning frameworks like TensorFlow or PyTorch are more optimized for such tasks due to their efficient parameter synchronization and communication, making them better choices for complex model-parallel workloads.

## Question 6: ANN

1. Why a classical Perceptron (i.e., a single layer of linear threshold units) is not preferable to use?

   - Perceptron is not preferable to use because it is not able to learn the XOR function. It is only able to learn linearly separable functions.

2. Implement a feedforward neural network by using the Keras API in TensorFlow for a regression problem.
   Assume that the data set has four numerical features and one numerical target variable. The network has one
   hidden layer with the sigmoid activation function. The number of neurons in the hidden layer is considered as
   a hyperparameter which you need to fine-tune. Write down the Python code of the implementation.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def create_regression_model(X_train, y_train, hidden_layer_neurons=32, learning_rate=0.001, epochs=100, batch_size=32):
    """
    Create and train a feedforward neural network for regression using Keras in TensorFlow.

    Args:
        X_train (numpy.ndarray): Training features.
        y_train (numpy.ndarray): Training target variable.
        hidden_layer_neurons (int): Number of neurons in the hidden layer.
        learning_rate (float): Learning rate for optimization.
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.

    Returns:
        keras.Sequential: Trained Keras model.
    """

    # Standardize the features (optional but recommended)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)

    # Create a sequential model
    model = tf.keras.Sequential()

    # Add the input layer
    model.add(tf.keras.layers.Input(shape=(X_train.shape[1],)))

    # Add the hidden layer with sigmoid activation
    model.add(tf.keras.layers.Dense(hidden_layer_neurons, activation='sigmoid'))

    # Add the output layer (one neuron for regression)
    model.add(tf.keras.layers.Dense(1))

    # Compile the model
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mean_squared_error',  # For regression problems
                  metrics=['mse'])  # Optional: Monitor mean squared error during training

    # Train the model
    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

    return model
```
