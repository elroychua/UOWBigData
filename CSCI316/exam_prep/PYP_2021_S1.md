# Final Examination Paper Session 1 2021

## Question 1

#### 1. Implement from scratch a Python function for simple numerical encoding. This function takes a list of string values as input and returns a vector of integers as output. Write down the Python code.

```python
# Simple Numerical Encoding
def simpleNumericalEncoding(data):
    unique_values = list(set(data)) # get unique values
    label_to_index = {}
    for i, label in unique_values:
        label_to_index[label] = i
    encoded_data = [label_to_index[label] for label in data]
    return encoded_data
```

#### 2. Implement from scratch a Python function to compute the Gini index of a list. This function takes a list of categorical values as input and returns the Gini index as output. Write down the Python code.

```python
# Gini Index
# Categorical input means that the input is without any inherent order or ranking.
def giniIndex(data):
    total_count = len(data)
    if total_count == 0:
        return 0
    class_counts = {}
    for value in data:
        if value in class_counts:
            class_counts[value] += 1
        else:
            class_counts[value] = 1

    gini_index = 1.0

    for class_count in class_counts.values():
        gini_index -= (class_count / total_count) ** 2

    return gini_index

data = ["A", "A", "B", "B", "C"]
print(giniIndex(data))
# Lower gini means data set is purer
# Closer to 1 means data set is more impure

```

## Question 2

1. Explain why pre-processing is important in big data.</br>
   Pre-processing is import in big data because through data transformation, data quality, data format and size of the data are, eliminating outliers, missing values and noise also factored in. Training and testing data are scaled and transformed. Categorical data can be transformed into numerical data without losing their original characteristics. With the use of correlation matrix, we can get feature selection, relevant features can be selected to improve the performance of the model. </br>

   - Negative correlation means that when one attribute goes up, the other goes down.
   - Positive correlation means that when one attribute goes up, the other goes up.</br>

2. Explain the advantages and disadvantages of data aggregation.</br>
   Data Aggregation is a process where raw data is gathered and expressed in a summary form or grouping. </br>

- Advantages: Easier to discover patterns, Reduced storage space, input/output time & execution time

- Disadvantages: Loss of relevant information, data accuracy

3. Explain undersampling and oversampling, and when you will apply them.
   Deal with class imbalance in a dataset.
   - Undersampling is a technique used to balance the class distribution of a dataset by reducing the size of the class with more observations.
   - Oversampling is a technique used to balance the class distribution of a dataset by increasing the size of the class with fewer observations. </br>

## Question 3 - Decision Tree

1. Assume that you are given a set of records as shown in the following table, where the last column contains
   the target variable. Present the procedure of using Gain Ratio to identify which attribute should be split. You
   need to show all steps of your calculation in detail.

a. Calculate the entropy of the target variable (Student Satisfaction).
P(low)= 4/8 = 0.5
P(high) = 4/8 = 0.5

    Entropy(StudentSatisfaction) = -[P(low)* log2(P(low)) + P(high)* log2(P(high))]</br>
    Entropy(StudentSatisfaction) = -[0.5* log2(0.5) + 0.5* log2(0.5)] </br>
    Entropy(StudentSatisfaction) = 1

b. Calculate the information gain of each attribute.

- Lecturer Experience
  Split the dataset based on "Lecturer experience":</br>
  For "Strong," there are 4 instances (1, 5, 6, 7), and the target variable distribution is (3 Low, 1 High).
  For "Weak," there are 4 instances (2, 3, 4, 8), and the target variable distribution is (1 Low, 3 High).

  - Calculate the entropy for each split:
    Entropy(Strong) = -[P(low)* log2(P(low)) + P(high)* log2(P(high))]</br>
    Entropy(Strong) = -[0.75* log2(0.75) + 0.25* log2(0.25)] </br>
    Entropy(Strong) = 0.811</br>
    Entropy(Weak) = -[P(low)* log2(P(low)) + P(high)* log2(P(high))]</br>
    Entropy(Weak) = -[0.25* log2(0.25) + 0.75* log2(0.75)] </br>
    Entropy(Weak) = 0.811</br>
  - Calculate weighted entropy:
    WeightedEntropy(LecturerExperience) = (4/8) \* Entropy(Strong) \* Entropy(Weak)
    WeightedEntropy(LecturerExperience) = 0.811
  - Calculate information gain:
    IG(LecturerExperience) = Entropy(StudentSatisfaction) - WeightedEntropy(LecturerExperience)
    IG(LecturerExperience) = 0.189

- Programming Subject
  Split the dataset based on "Programming Subject":</br>
  For "Yes," there are 4 instances (1, 2, 5, 6), and the target variable distribution is (2 Low, 2 High).
  For "No," there are 4 instances (3, 4, 7, 8), and the target variable distribution is (2 Low, 2 High).

  - Calculate the entropy for each split:
    Entropy(Yes) = -[P(low) * log2(P(low)) + P(high) * log2(P(high))]</br>
    Entropy(Yes) = -[0.5 * log2(0.5) + 0.5 * log2(0.5)] </br>
    Entropy(Yes) = 1</br>
    Entropy(No) = -[P(low) * log2(P(low)) + P(high) * log2(P(high))]</br>
    Entropy(No) = -[0.5 * log2(0.5) + 0.5 * log2(0.5)] </br>
    Entropy(No) = 1</br>
  - Calculate weighted entropy:
    WeightedEntropy(ProgrammingSubject) = (4/8) \* Entropy(Yes) \* Entropy(No)
    WeightedEntropy(ProgrammingSubject) = 1
  - Calculate information gain:
    IG(ProgrammingSubject) = Entropy(StudentSatisfaction) - WeightedEntropy(ProgrammingSubject)
    IG(ProgrammingSubject) = 0

- c. Calculate split information of each attribute.

  - SplitInformation(LecturerExperience) = -[P(Strong) * log2(P(Strong)) + P(Weak) * log2(P(Weak))]</br>
    SplitInformation(LecturerExperience) = -[0.5 * log2(0.5) + 0.5 * log2(0.5)] </br>
    SplitInformation(LecturerExperience) = 1</br>
  - SplitInformation(ProgrammingSubject) = -[P(Yes) * log2(P(Yes)) + P(No) * log2(P(No))]</br>
    SplitInformation(ProgrammingSubject) = -[0.5 * log2(0.5) + 0.5 * log2(0.5)] </br>
    SplitInformation(ProgrammingSubject) = 1</br>
    SplitInformation(StudentSatisfaction) = 1</br>

- d. Calculate gain ratio of each attribute.

  - GainRatio(LecturerExperience) = IG(LecturerExperience) / SplitInformation(LecturerExperience)
    GainRatio(LecturerExperience) = 0.189 / 1
    GainRatio(LecturerExperience) = 0.189
  - GainRatio(ProgrammingSubject) = IG(ProgrammingSubject) / SplitInformation(ProgrammingSubject)
    GainRatio(ProgrammingSubject) = 0 / 1
    GainRatio(ProgrammingSubject) = 0

- e. Which attribute should be split?
  LecturerExperience should be split because it has the highest gain ratio.

2. Why an ensemble classifier (such as a Random Forest) can enhance the performance of individual
   classifiers?</br>
   Ensemble classifiers like Random Forests combine the strengths of multiple base learners to improve overall predictive performance, reduce overfitting, and enhance model robustness. They are a powerful tool for a wide range of machine learning tasks and are often used when accuracy and generalization are critical.

## Question 4 - Naive Bayes

(Issue with Bayes theorem is that it assumes that all features are independent of each other, which is not always the case. Naive Bayes is a classifier that assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.)

1. Use an example to illustrate the conditional independence assumption, and explain why it is important to the Naïve Bayes classifier.</br>
   The conditional independence assumption that each attribute is conditionaly not dependent of another attribute in this context means that the presence of "free" in an email is independent of the presence of "discount," given the class label (spam or not spam). In other words, knowing that "free" is in the email does not provide any additional information about whether "discount" is in the email when we already know if it's spam or not.

2. Assume that a Bayesian classifier returns the following outcomes for a binary classification problem, which are sorted by decreasing probability values. P (resp., N) refers to a record belonging to a positive (resp., negative) class.

   a. What are the true positive (recognition) rate and false negative (recognition) rate if setting the probabilistic classification threshold to ≥ 0.70?</br>
   Threshold ≥ 0.70:

   - Positive Predictions: Tuples 1 (P) and 3 (P).
   - Negative Predictions: Tuples 4 (N) to 10 (N). </br>
     True Positive Rate = 2/(2) = 1 </br>
     False Negative Rate = 0</br>
     So, with a threshold of ≥ 0.70: True Positive Rate = 1 (100%) & False Negative Rate = 0 (0%) because there are no false negatives.

   b. What is the smallest probabilistic classification threshold such that the precision is at least 60%? </br>
   To find the smallest probabilistic classification threshold such that precision is at least 60%, we need to consider different thresholds and calculate precision at each step. Precision is calculated as:
   Precision = TP / (TP + FP)

   - Threshold >= 0.90:
     Precision = 1 / (1 + 0) = 1
   - Threshold >= 0.80:
     Precision = 2 / (2 + 0) = 1
   - Threshold >= 0.70:
     Precision = 2 / (2 + 1) = 0.67
   - Threshold >= 0.60:
     Precision = 3 / (3 + 0) = 0.75
   - Threshold >= 0.55:
     Precision = 3 / (4 + 1) = 0.6
   - Threshold >= 0.54:
     Precision = 4 / (4 + 2) = 0.67
   - Threshold >= 0.53:
     Precision = 5 / (5 + 2) = 0.71

     The smallest probabilistic classification threshold such that the precision is at least 60% is 0.54.

## Question 5 - Analytics with Apache Spark

1. Use an example to explain how the MapReduce model can process the outer join operation.

Outer Join

- Given two datasets, A and B, the outer join operation returns a new dataset that contains all the tuples in A and B. If a tuple in A does not have a matching tuple in B, the attributes of the tuple in B are set to NULL. Similarly, if a tuple in B does not have a matching tuple in A, the attributes of the tuple in A are set to NULL.
- Using MapReduce, the outer join operation can be performed by first performing a full outer join on the join attribute, and then filtering out the tuples that do not satisfy the join condition such as not equal to NULL.

2. Why Apache Spark is suitable for large-scale machine learning? Use an example to support your answer.

- Spark is designed for distributed data processing, which means it can efficiently handle large volumes of data by distributing it across a cluster of machines.
- Spark's core data structure, RDDs, allows for fault-tolerant distributed processing. It can recover from node failures, ensuring that large-scale machine learning jobs continue without interruption.
- Spark provides high-level APIs, including the MLlib library, which simplifies the implementation of machine learning algorithms.
- Spark incorporates various performance optimizations, such as query optimization, data caching, and lazy evaluation, to improve the speed of data processing and machine learning tasks.
- Spark scales horizontally, meaning you can add more machines to the cluster as your data and computation requirements grow.

3. Assume that a DataFrame named FlightsDF of flight statistics is defined in PySpark, with the following code processed.</br>

   ````
   FlightsDF.printSchema()</br>
   Out:</br>
   root</br>
   |-- DEST_CITY: string (nullable = true)</br>
   |-- DEST_COUNTRY_NAME: string (nullable = true)</br>
   |-- ORIGIN_CITY: string (nullable = true)</br>
   |-- ORIGIN_COUNTRY_NAME: string (nullable = true)</br>```


   DF.show(2)
   Out:
   +----------------------+-----------+--------------+
   |DEST_CITY|DEST_COUNTRY|ORIGIN_CITY|ORIGIN_COUNTRY|
   +---------+------------+-----------+--------------+
   |Sydney |Australia |Melbourne |Australia |
   |Auckland |New Zealand |Singapore |Singapore |
   +---------+------------+-----------+--------------+
   ````

   only showing top 2 rows</br>
   Based on FlightsDF, write down the code in PySpark to implement the following operation: Find the country or countries with most international flights. (Note. An international flights has different original and destination countries.)

   ```python
   from pyspark.sql import SparkSession
   from pyspark.sql.functions import col, when, count

   # Initialize Spark session
   spark = SparkSession.builder.appName("InternationalFlights").getOrCreate()
   # Assuming you already have the FlightsDF DataFrame

   # Step 1: Calculate the total count of international flights for each country pair
   international_flights_df = FlightsDF.withColumn(
       "is_international",
       when(col("DEST_COUNTRY_NAME") != col("ORIGIN_COUNTRY_NAME"), 1).otherwise(0)
   )

   # Step 2: Filter out domestic flights
   international_flights_df = international_flights_df.filter(col("is_international") == 1)

   # Step 3: Aggregate and find the countries with the highest international flight counts
   country_counts_df = international_flights_df.groupBy("DEST_COUNTRY_NAME").agg(count("*").alias("flight_count"))

   # Find the country or countries with the most international flights
   max_flight_count = country_counts_df.agg({"flight_count": "max"}).collect()[0][0]

   # Filter countries with the highest international flight counts
   countries_with_most_flights_df = country_counts_df.filter(col("flight_count") == max_flight_count)

   # Show the result
   countries_with_most_flights_df.show()

   # Stop the Spark session
   spark.stop()

   ```

## Question 6 - Artificial Neural Networks

1. Why a classical Perceptron (i.e., a single layer of linear threshold units) is not preferable to use?

- The perceptron is a single layer of LTUs.
- While classical Perceptrons played a crucial historical role in the development of neural networks and machine learning, they are now considered primitive compared to more advanced neural network architectures, such as multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs). These modern architectures overcome many of the limitations of classical Perceptrons and have led to significant advancements in various machine learning tasks, including image recognition, natural language processing, and reinforcement learning.

2. Implement a feedforward neural network by using the Keras API in TensorFlow for a multi-class classification problem. Assume that the data set has four numerical features and one target variable whose values are 1, 2 and 3. The network has one hidden layer with the sigmoid activation function. Present the Python code.

```python
# Creating a feedforward neural network using Keras API in TensorFlow, if 3D need to .flatten first
# Importing the required libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Creating the model
model = keras.Sequential(
    [
        layers.Dense(4, activation="sigmoid", name="layer1"),
        layers.Dense(3, activation="softmax", name="layer2"),
    ]
)

# Compiling the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Training the model
model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1)

# Evaluating the model
model.evaluate(x_test, y_test, verbose=1)

```
