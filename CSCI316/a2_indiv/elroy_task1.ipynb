{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 Task 1\n",
    "\n",
    "<strong>Name:</strong> Elroy Chua Ming Xuan </br>\n",
    "<strong>UOW ID: </strong> 7431673 </br>\n",
    "<strong>Data set: </strong> https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Necessary Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
    "test_df = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (consider other strategies like imputation)\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Convert categorical variables using get_dummies\n",
    "train_df = pd.get_dummies(train_df, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Sample 20% of the training data\n",
    "train_df = train_df.sample(frac=0.2)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df.drop('Churn', axis=1)\n",
    "y_train = train_df['Churn']\n",
    "X_test = test_df.drop('Churn', axis=1)\n",
    "y_test = test_df['Churn']\n",
    "\n",
    "# Convert DataFrames to Numpy Arrays\n",
    "train_data = np.array(pd.concat([X_train, y_train], axis=1))\n",
    "test_data = np.array(pd.concat([X_test, y_test], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Implement Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "def entropy(data):\n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def information_gain(data_left, data_right, current_entropy):\n",
    "    p = float(len(data_left) / (len(data_left) + len(data_right)))\n",
    "    return current_entropy - p * entropy(data_left) - (1 - p) * entropy(data_right)\n",
    "\n",
    "\n",
    "def split_check(data):\n",
    "    feature_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):\n",
    "        unique_values = np.unique(data[:, column_index])\n",
    "        feature_splits[column_index] = unique_values\n",
    "    return feature_splits\n",
    "\n",
    "\n",
    "def InfoGainSplit(data, potential_splits):\n",
    "    current_entropy = entropy(data)\n",
    "    best_info_gain = -1\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_left, data_right = split(\n",
    "                data, split_column=column_index, split_value=value)\n",
    "            current_info_gain = information_gain(\n",
    "                data_left, data_right, current_entropy)\n",
    "            if current_info_gain >= best_info_gain:\n",
    "                best_info_gain = current_info_gain\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "\n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "def split(data, split_column, split_value):\n",
    "    split_column_values = data[:, split_column]\n",
    "    data_left = data[split_column_values <= split_value]\n",
    "    data_right = data[split_column_values > split_value]\n",
    "    return data_left, data_right\n",
    "\n",
    "\n",
    "def classify(data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(\n",
    "        label_column, return_counts=True)\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    return classification\n",
    "\n",
    "\n",
    "def check_purity(data):\n",
    "    unique_classes = np.unique(data[:, -1])\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def DT(data, depth=0, max_depth=10):\n",
    "    if len(data) == 0:\n",
    "        return None  # Return None for empty data\n",
    "    if check_purity(data) or depth >= max_depth:\n",
    "        return classify(data)\n",
    "    else:\n",
    "        potential_splits = split_check(data)\n",
    "        if not potential_splits:  # Check if potential_splits is empty\n",
    "            return classify(data)\n",
    "        split_column, split_value = InfoGainSplit(data, potential_splits)\n",
    "        data_left, data_right = split(data, split_column, split_value)\n",
    "\n",
    "        question = \"{} <= {}\".format(split_column, split_value)\n",
    "        subtree = {question: []}\n",
    "        yes_answer = DT(data_left, depth=depth+1, max_depth=max_depth) # Increment depth for recursive call\n",
    "        no_answer = DT(data_right, depth=depth+1, max_depth=max_depth) # Increment depth for recursive call\n",
    "        if len(data_left) >= len(data) or len(data_right) >= len(data):\n",
    "            return classify(data)\n",
    "        if yes_answer == no_answer:\n",
    "            subtree = yes_answer\n",
    "        else:\n",
    "            subtree[question].append(yes_answer)\n",
    "            subtree[question].append(no_answer)\n",
    "\n",
    "        return subtree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "tree = DT(train_data, max_depth=10)  # Add max_depth parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 49.75%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Decision Tree\n",
    "correct_tree = 0\n",
    "for i in range(len(test_data)):\n",
    "    prediction_tree = predict(test_data[i], tree)\n",
    "    if prediction_tree == test_data[i][-1]:\n",
    "        correct_tree += 1\n",
    "\n",
    "accuracy_tree = correct_tree / len(test_data)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_tree * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions with Trained Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature, comparison, value = question.split(\" \")\n",
    "\n",
    "    # Check for boolean values\n",
    "    if value == \"True\":\n",
    "        comparison_value = True\n",
    "    elif value == \"False\":\n",
    "        comparison_value = False\n",
    "    else:\n",
    "        comparison_value = float(value)\n",
    "\n",
    "    # Ask question\n",
    "    if example[int(feature)] <= comparison_value:\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # Base case: If answer is not another dict, return it (leaf node)\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "\n",
    "    # Recursive part: Go deeper in the tree\n",
    "    return predict(example, answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Accuracy of the Trained Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 49.75%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(test_data)):\n",
    "    prediction = predict(test_data[i], tree)\n",
    "    if prediction == test_data[i][-1]:  # Assuming the label is the last column\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(test_data)\n",
    "print(f\"Decision Tree Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n",
    "1. Bootstrapping\n",
    "2. Feature Selection\n",
    "3. Tree Construction\n",
    "4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping \n",
    "def bootstrap_sample(data, bootstrap_size=None):\n",
    "    if bootstrap_size is None:\n",
    "        bootstrap_size = len(data)\n",
    "    indices = np.random.choice(len(data), size=bootstrap_size, replace=True)\n",
    "    return data[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "def random_features(data, num_features):\n",
    "    total_features = data.shape[1] - 1  # subtracting one for the label column\n",
    "    features = np.random.choice(total_features, num_features, replace=False)\n",
    "    return np.column_stack([data[:, i] for i in features] + [data[:, -1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Random Forest\n",
    "def train_random_forest(data, num_trees, num_features):\n",
    "    forest = []\n",
    "    for _ in range(num_trees):\n",
    "        bootstrap_data = bootstrap_sample(data)\n",
    "        bootstrap_data = random_features(bootstrap_data, num_features)\n",
    "        tree = DT(bootstrap_data)\n",
    "        forest.append(tree)\n",
    "    return forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with Random Forest\n",
    "def random_forest_predict(forest, example):\n",
    "    tree_predictions = [predict(example, tree) for tree in forest]\n",
    "    return max(set(tree_predictions), key=tree_predictions.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 50.15%\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "num_trees = 10\n",
    "num_features = 5\n",
    "forest = train_random_forest(train_data, num_trees, num_features)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "correct_forest = 0\n",
    "for i in range(len(test_data)):\n",
    "    prediction_forest = random_forest_predict(forest, test_data[i])\n",
    "    if prediction_forest == test_data[i][-1]:\n",
    "        correct_forest += 1\n",
    "\n",
    "accuracy_forest = correct_forest / len(test_data)\n",
    "print(f\"Random Forest Accuracy: {accuracy_forest * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 49.75%\n",
      "Random Forest Accuracy: 50.15%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decision Tree Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_forest * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison:\n",
    "Based on the two models create above, we can see that Random Forest Accuracy is 50.15 and Decision Tree Accuracy is 49.85. This shows that Random Forest is a better model to use for this dataset. This is because Random Forest is an ensemble model that uses multiple decision trees to make predictions. This means that it is more accurate than a single decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
