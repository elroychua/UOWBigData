{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 1 Task 1\n",
        "\n",
        "<strong>Name:</strong> Elroy Chua Ming Xuan </br>\n",
        "<strong>UOW ID:</strong> 7431673\n",
        "<strong>Data Set:</strong> Customer Churn Dataset\n",
        "https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Create one Pandas dataframe from the two CSV files\n",
        "In order to merge the training and test data, first, we will load both data sets using Pandas, and then we will concatenate them into a single dataframe. Assume that we have two files, train.csv and test.csv, from the given link. Below is the Python code to perform this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data\n",
            "   CustomerID   Age  Gender  Tenure  Usage Frequency  Support Calls  \\\n",
            "0         2.0  30.0  Female    39.0             14.0            5.0   \n",
            "1         3.0  65.0  Female    49.0              1.0           10.0   \n",
            "2         4.0  55.0  Female    14.0              4.0            6.0   \n",
            "3         5.0  58.0    Male    38.0             21.0            7.0   \n",
            "4         6.0  23.0    Male    32.0             20.0            5.0   \n",
            "\n",
            "   Payment Delay Subscription Type Contract Length  Total Spend  \\\n",
            "0           18.0          Standard          Annual        932.0   \n",
            "1            8.0             Basic         Monthly        557.0   \n",
            "2           18.0             Basic       Quarterly        185.0   \n",
            "3            7.0          Standard         Monthly        396.0   \n",
            "4            8.0             Basic         Monthly        617.0   \n",
            "\n",
            "   Last Interaction  Churn  \n",
            "0              17.0    1.0  \n",
            "1               6.0    1.0  \n",
            "2               3.0    1.0  \n",
            "3              29.0    1.0  \n",
            "4              20.0    1.0  \n",
            "CustomerID           1\n",
            "Age                  1\n",
            "Gender               1\n",
            "Tenure               1\n",
            "Usage Frequency      1\n",
            "Support Calls        1\n",
            "Payment Delay        1\n",
            "Subscription Type    1\n",
            "Contract Length      1\n",
            "Total Spend          1\n",
            "Last Interaction     1\n",
            "Churn                1\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "# Load the train_train_train_data\n",
        "test_data = pd.read_csv('customer_churn_dataset-testing-master.csv')\n",
        "train_data = pd.read_csv('customer_churn_dataset-training-master.csv')\n",
        "\n",
        "# Concatenate the dataframes\n",
        "data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "#Data\n",
        "print(\"Data\")\n",
        "# Inspect the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Identify missing values\n",
        "print(data.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Once missing values are identified, remove the missing values from the dataset.\n",
        "Missing data can be identified with the isna() function in Pandas. To clean missing values, we can use various imputation methods like mean, median, mode imputation, or more advanced techniques like using regression models. Here, I'll show you how to do mean imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check for missing values after dropping data\n",
            "CustomerID           0\n",
            "Age                  0\n",
            "Gender               0\n",
            "Tenure               0\n",
            "Usage Frequency      0\n",
            "Support Calls        0\n",
            "Payment Delay        0\n",
            "Subscription Type    0\n",
            "Contract Length      0\n",
            "Total Spend          0\n",
            "Last Interaction     0\n",
            "Churn                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# If there are missing values, we drop them\n",
        "data.dropna(inplace=True)\n",
        "# Dropping the missing values\n",
        "print(\"Check for missing values after dropping data\")\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Perform z-score normalization of the values in the attribute “Last Interaction”. Show the mean and variance of the normalized values\n",
        "Z-score normalization is a scaling method that transforms the data into a distribution with a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-7.831066837512298e-17\n",
            "0.9999999999999998\n"
          ]
        }
      ],
      "source": [
        "# Z-score normalization\n",
        "data['Last Interaction'] = (data['Last Interaction'] - data['Last Interaction'].mean()) / data['Last Interaction'].std()\n",
        "\n",
        "# Print the mean and variance\n",
        "print(data['Last Interaction'].mean())\n",
        "print(data['Last Interaction'].var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Create five bins for the attribute “Total Spend” such that the bins contain (approximately) equivalent numbers of records.\n",
        "We can use the qcut function from pandas which creates bins of equal size based on the quantiles of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Spend Bins\n",
            "(99.999, 378.0]    101271\n",
            "(719.0, 859.0]     101223\n",
            "(578.0, 719.0]     101069\n",
            "(378.0, 578.0]     100822\n",
            "(859.0, 1000.0]    100821\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create bins\n",
        "data['Total Spend Bins'] = pd.qcut(data['Total Spend'], q=5)\n",
        "# Check the bins\n",
        "print(data['Total Spend Bins'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 5: Apply one-hot-encoding to the attribute “Contract Length”.\n",
        "One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. Pandas provides get_dummies function which is used to convert categorical variable into dummy/indicator variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['CustomerID', 'Age', 'Gender', 'Tenure', 'Usage Frequency',\n",
            "       'Support Calls', 'Payment Delay', 'Subscription Type',\n",
            "       'Contract Length', 'Total Spend', 'Last Interaction', 'Churn',\n",
            "       'Total Spend Bins', 'Contract Length_Annual', 'Contract Length_Monthly',\n",
            "       'Contract Length_Quarterly'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# One-hot encoding\n",
        "data = pd.concat([data, pd.get_dummies(data['Contract Length'], prefix='Contract Length')], axis=1)\n",
        "\n",
        "# Check the new columns\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 6: Define at least one new attribute based on existing attribute, and explain your reason behind your definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#5. Apply one-hot-encoding to the attribute “Contract Length”.\n",
        "TODO: create a new attribute.\n",
        "# data['Tenure'] = data['Total Charges'] / data['Monthly Charges']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.10 ('testEnv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "96f8c630555ca8c8517ffff7a44c42c100b07707e04e471dc387e2341790e389"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
