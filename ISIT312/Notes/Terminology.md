# Necessary Terminology:

HDFS (Hadoop Distributed File System): HDFS is a distributed file system designed to store and manage large datasets across multiple machines in a cluster. It provides high throughput access to data and ensures fault-tolerance by replicating data across multiple nodes. HDFS is the primary storage layer in Hadoop and is responsible for storing the input and output data for MapReduce jobs.

YARN (Yet Another Resource Negotiator): YARN is the cluster resource management framework in Hadoop. It allows multiple processing engines, such as MapReduce, Apache Spark, and Apache Flink, to run concurrently on the same cluster. YARN manages and allocates cluster resources (CPU, memory, etc.) to different applications running on the cluster, providing a multi-tenant environment for running various workloads.

MapReduce: MapReduce is a programming model and processing framework for large-scale data processing in parallel across a Hadoop cluster. It is based on the idea of dividing a computation into two phases: the map phase and the reduce phase. The map phase processes input data in parallel across multiple nodes, and the reduce phase aggregates the intermediate results generated by the map phase to produce the final output. MapReduce is designed to handle distributed processing of large datasets across a cluster of machines.

## Phases in MapReduce Process:
1. Data Loading: In this phase, the input data is loaded into the Hadoop Distributed File System (HDFS) or any other storage system supported by Hadoop. The input data is typically divided into multiple splits, and each split is processed independently by a map task.

2. Map Phase: In this phase, each map task processes a subset of the input data independently. The map task takes the input key-value pairs and applies a user-defined map function to generate intermediate key-value pairs. The map function processes the input data and produces intermediate results specific to each input record. The output of the map phase is a set of intermediate key-value pairs.

3. Partitioning and Sorting: After the map phase, the intermediate key-value pairs are partitioned based on the keys and sent to the reducers. The partitioning ensures that all key-value pairs with the same key end up in the same reducer. Additionally, the intermediate key-value pairs are sorted within each partition based on the keys. Sorting is important to efficiently group the records with the same key during the reduce phase.

4. Reduce Phase: In this phase, each reducer receives a partition of the sorted intermediate key-value pairs. The reduce task applies a user-defined reduce function to the input key-value pairs and produces the final output. The reduce function typically aggregates the values associated with each key to produce the final output for that key. The output of the reduce phase is the final result of the MapReduce job.

## Hive Concepts:
Hive is a data warehouse infrastructure built on top of Hadoop. It provides a higher-level abstraction and a SQL-like query language called HiveQL (HQL) to query and analyze large datasets stored in Hadoop Distributed File System (HDFS) or other compatible file systems. Hive allows users to leverage the power of Hadoop for data processing and analysis using familiar SQL-like syntax.

1. Create table:
```sql
CREATE TABLE employees (
employeeId VARCHAR(3),
fullName  VARCHAR(60),
projects MAP<STRING, INT>,
languages ARRAY<STRING>)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
LOCATION 'location';
```

2. Insert data:
```sql
INSERT INTO employees
SELECT '007', 'James Bond', map('DB/3', 30, 'Oracle', 25, 'SQL-2022', 100), array('Java','C','C++');
INSERT INTO employees
SELECT '008', 'James Bond', map('DB/3', 70, 'Oracle', 100), array();
```

3. Get data:
```sql
SELECT AVG(driver_count) as average_drivers_per_truck
FROM (
    SELECT t.t_rego, COUNT(DISTINCT c.d_license) AS driver_count
    FROM truck 
    JOIN company c ON t.t_rego = c.t_rego
    GROUP BY t.t_rego
) subquery;
```

# Data Warehouse
A data warehouse is a central repository of integrated data from multiple sources that is designed to support business intelligence and reporting activities.

Data warehouses follow a multi-layered architecture known as the Data Warehouse Architecture

Data Source Layer: This layer represents the various data sources from which data is extracted to populate the data warehouse.

Data Integration Layer: The data from different sources is extracted, transformed, and loaded (ETL) into the data warehouse.(data cleansing, data transformation, and data aggregation).

Data Storage Layer: This layer is where the data warehouse physically stores the integrated and transformed data. It typically consists of two types of tables: fact tables and dimension tables.

Fact Tables: Fact tables contain the core business metrics or measures that represent the numerical values to be analyzed, such as sales revenue, quantity sold, or customer count. Fact tables also include foreign keys that link to dimension tables.

Dimension Tables: Dimension tables provide descriptive information about the business entities, such as customers, products, time, and geography. They contain attributes that help categorize and filter the data in the fact table.

Data Access Layer: This layer provides various tools and interfaces for users to access and analyze the data stored in the data warehouse. It includes reporting tools, business intelligence platforms, and ad-hoc query tools that allow users to run queries, generate reports, and perform data analysis.